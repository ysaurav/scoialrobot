![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo3.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/logo3.png)


# Torso Orientation #
![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/met6.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/met6.png)

## Introduction ##

The pose estimation of the head can be estimated from pixel location in the image by using the extrinsic and intrinsic parameters of the camera calibration. However, the angle that describes torso orientation can be a challenging problem, if a 3D model is not available.

One of the advantages of kinect, is the generation of range images that provide 3D information. In this site, we will explain how to use this information to compute the angle of torso orientation by using a very straighforward algorithm.

## Algorithm ##

This algorithm is carried out by using the depth image generated by Kinect. However, a disparity image is used in order to illustrate every step of this procedure.

### Inputs: ###

The inputs of this algorithm is the 2D coordinates of the centroid (x,y) generated by the face detection (see the below right-hand side picture )

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step1.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step1.png)

### Step 1: ###

Starting from the centroids, we look for the maximum variation of pixel intensity values, which represent the depth of the image. We take the vertical direction in order to reach the superior part of the head. A significant variation is assumed to be located at the border between the human and the background, specifically, to a 'x' distance from the detected centroid. Hence, we can stablish clearly the top boundary. The following figure illustrates this step for two detected persons.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step2.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step2.png)

### Step 2: ###

Based on proportion estimates, we look for a point in the torso region that allows to have a better estimation of the orientation. For that reason, we move a 1.5x distance from the centroid in the opposite direction (see green lines on the below figure)

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step3.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step3.png)

### Step 3: ###

Once the torso region has been reached, we proceed to find the side boundaries by moving left and right in a horizontal direction.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step5.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step5.png)

### Step 4: ###

A rectangle that encloses the detected person is computed from the top and side boundaries. To determine the width of the rectangle, we consider distances from centroid to left and right side boundaries. We called these distances d1 and d2, respectively. So,the width is computed from the minimum distance between d1 and d2, by applying symmetry with respect to the centroid.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step7.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step7.png)

### Step 5: ###

The width of the rectangle is divided in four parts in order to select the points corresponding to one-fourth and three-fourths of the person. These are points which are close enough to the boundaries, and their depth information is less noisy.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step8.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step8.png)

### Step 6: ###

Finally, these points are identified in the final image.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step9.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step9.png)

### Step 7: ###

Once these points have been transformed in real world, we proceed to compute the angle of torso orientation by using the following formula.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step10.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/step10.png)

## Results ##
The angle computed for this person is equal to -7.03 degrees.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/torso1.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/torso1.png)

The angle computed for the left person is -21.04 degrees and the right person is 17.85 degrees.

![https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/torso2.png](https://scoialrobot.googlecode.com/svn/trunk/social_robot/pictures/torso2.png)

## Results ##

<a href='http://www.youtube.com/watch?feature=player_embedded&v=VG3M6QSxng0' target='_blank'><img src='http://img.youtube.com/vi/VG3M6QSxng0/0.jpg' width='425' height=344 /></a>

## Limitations ##

  1. The algorithm can not deal with problems of occlusion. It means, when there are persons on the scene that appears overlapped among each other, there is not enough information to differentiate them.
  1. When the detected persons appear to be located at a far distance from the kinect, their depth information is very close to background. Hence, the computed angle can be overestimated.
  1. If the detected person assumes very particular poses, the angle of orientation could not give an accurate estimate.